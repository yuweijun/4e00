<!DOCTYPE html>

<HTML><head><TITLE>Manpage of WWW::RobotRules</TITLE>
<meta charset="utf-8">
<link rel="stylesheet" href="/css/main.css" type="text/css">
</head>
<body>
 <header class="site-header">
 <div class="wrap"> <div class="site-title"><a href="/manpages/index.html">linux man pages</a></div>
 <div class="site-description">{"type":"programming"}</div>
 </div>
 </header>
 <div class="page-content">
<H1>WWW::RobotRules</H1>
Section: User Contributed Perl Documentation (3)<BR>Updated: 2009-10-03<BR><A HREF="#index">Index</A>
<A HREF="/manpages/index.html">Return to Main Contents</A><HR>






<A NAME="lbAB">&nbsp;</A>
<H2>NAME</H2>

WWW::RobotRules - database of robots.txt-derived permissions
<A NAME="lbAC">&nbsp;</A>
<H2>SYNOPSIS</H2>

<A NAME="ixAAC"></A>


<PRE>
 use WWW::RobotRules;
 my $rules = WWW::RobotRules-&gt;new('MOMspider/1.0');

 use LWP::Simple qw(get);

 {
   my $url = &quot;<A HREF="http://some.place/robots.txt">http://some.place/robots.txt</A>&quot;;
   my $robots_txt = get $url;
   $rules-&gt;parse($url, $robots_txt) if defined $robots_txt;
 }

 {
   my $url = &quot;<A HREF="http://some.other.place/robots.txt">http://some.other.place/robots.txt</A>&quot;;
   my $robots_txt = get $url;
   $rules-&gt;parse($url, $robots_txt) if defined $robots_txt;
 }

 # Now we can check if a URL is valid for those servers
 # whose &quot;robots.txt&quot; files we've gotten and parsed:
 if($rules-&gt;allowed($url)) {
     $c = get $url;
     ...
 }

</PRE>


<A NAME="lbAD">&nbsp;</A>
<H2>DESCRIPTION</H2>

<A NAME="ixAAD"></A>
This module parses <I>/robots.txt</I> files as specified in
``A Standard for Robot Exclusion'', at
&lt;<A HREF="http://www.robotstxt.org/wc/norobots.html">http://www.robotstxt.org/wc/norobots.html</A>&gt;
Webmasters can use the <I>/robots.txt</I> file to forbid conforming
robots from accessing parts of their web site.
<P>

The parsed files are kept in a WWW::RobotRules object, and this object
provides methods to check if access to a given <FONT SIZE="-1">URL</FONT> is prohibited.  The
same WWW::RobotRules object can be used for one or more parsed
<I>/robots.txt</I> files on any number of hosts.
<P>

The following methods are provided:
<DL COMPACT>
<DT>$rules = WWW::RobotRules-&gt;new($robot_name)<DD>


<A NAME="ixAAE"></A>
This is the constructor for WWW::RobotRules objects.  The first
argument given to <I>new()</I> is the name of the robot.
<DT>$rules-&gt;parse($robot_txt_url, $content, $fresh_until)<DD>


<A NAME="ixAAF"></A>
The <I>parse()</I> method takes as arguments the <FONT SIZE="-1">URL</FONT> that was used to
retrieve the <I>/robots.txt</I> file, and the contents of the file.
<DT>$rules-&gt;allowed($uri)<DD>


<A NAME="ixAAG"></A>
Returns <FONT SIZE="-1">TRUE</FONT> if this robot is allowed to retrieve this <FONT SIZE="-1">URL</FONT>.
<DT>$rules-&gt;agent([$name])<DD>


<A NAME="ixAAH"></A>
Get/set the agent name. <FONT SIZE="-1">NOTE:</FONT> Changing the agent name will clear the robots.txt
rules and expire times out of the cache.
</DL>
<A NAME="lbAE">&nbsp;</A>
<H2>ROBOTS.TXT</H2>

<A NAME="ixAAI"></A>
The format and semantics of the ``/robots.txt'' file are as follows
(this is an edited abstract of
&lt;<A HREF="http://www.robotstxt.org/wc/norobots.html">http://www.robotstxt.org/wc/norobots.html</A>&gt;):
<P>

The file consists of one or more records separated by one or more
blank lines. Each record contains lines of the form
<P>



<PRE>
  &lt;field-name&gt;: &lt;value&gt;

</PRE>


<P>

The field name is case insensitive.  Text after the '#' character on a
line is ignored during parsing.  This is used for comments.  The
following &lt;field-names&gt; can be used:
<DL COMPACT>
<DT>User-Agent<DD>
<A NAME="ixAAJ"></A>
The value of this field is the name of the robot the record is
describing access policy for.  If more than one <I>User-Agent</I> field is
present the record describes an identical access policy for more than
one robot. At least one field needs to be present per record.  If the
value is '*', the record describes the default access policy for any
robot that has not not matched any of the other records.


<P>


The <I>User-Agent</I> fields must occur before the <I>Disallow</I> fields.  If a
record contains a <I>User-Agent</I> field after a <I>Disallow</I> field, that
constitutes a malformed record.  This parser will assume that a blank
line should have been placed before that <I>User-Agent</I> field, and will
break the record into two.  All the fields before the <I>User-Agent</I> field
will constitute a record, and the <I>User-Agent</I> field will be the first
field in a new record.
<DT>Disallow<DD>
<A NAME="ixAAK"></A>
The value of this field specifies a partial <FONT SIZE="-1">URL</FONT> that is not to be
visited. This can be a full path, or a partial path; any <FONT SIZE="-1">URL</FONT> that
starts with this value will not be retrieved
</DL>
<P>

Unrecognized records are ignored.
<A NAME="lbAF">&nbsp;</A>
<H2>ROBOTS.TXT EXAMPLES</H2>

<A NAME="ixAAL"></A>
The following example ``/robots.txt'' file specifies that no robots
should visit any <FONT SIZE="-1">URL</FONT> starting with ``/cyberworld/map/'' or ``/tmp/'':
<P>



<PRE>
  User-agent: *
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space
  Disallow: /tmp/ # these will soon disappear

</PRE>


<P>

This example ``/robots.txt'' file specifies that no robots should visit
any <FONT SIZE="-1">URL</FONT> starting with ``/cyberworld/map/'', except the robot called
``cybermapper'':
<P>



<PRE>
  User-agent: *
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space

  # Cybermapper knows where to go.
  User-agent: cybermapper
  Disallow:

</PRE>


<P>

This example indicates that no robots should visit this site further:
<P>



<PRE>
  # go away
  User-agent: *
  Disallow: /

</PRE>


<P>

This is an example of a malformed robots.txt file.
<P>



<PRE>
  # robots.txt for ancientcastle.example.com
  # I've locked myself away.
  User-agent: *
  Disallow: /
  # The castle is your home now, so you can go anywhere you like.
  User-agent: Belle
  Disallow: /west-wing/ # except the west wing!
  # It's good to be the Prince...
  User-agent: Beast
  Disallow:

</PRE>


<P>

This file is missing the required blank lines between records.
However, the intention is clear.
<A NAME="lbAG">&nbsp;</A>
<H2>SEE ALSO</H2>

<A NAME="ixAAM"></A>
LWP::RobotUA, WWW::RobotRules::AnyDBM_File
<P>

<HR>
<A NAME="index">&nbsp;</A><H2>Index</H2>
<DL>
<DT><A HREF="#lbAB">NAME</A><DD>
<DT><A HREF="#lbAC">SYNOPSIS</A><DD>
<DT><A HREF="#lbAD">DESCRIPTION</A><DD>
<DT><A HREF="#lbAE">ROBOTS.TXT</A><DD>
<DT><A HREF="#lbAF">ROBOTS.TXT EXAMPLES</A><DD>
<DT><A HREF="#lbAG">SEE ALSO</A><DD>
</DL>
<HR>
This document was created by
<A HREF="/manpages/index.html">man2html</A>,
using the manual pages.<BR>
Time: 05:33:45 GMT, December 24, 2015
</div></body>
</HTML>
